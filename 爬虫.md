# 文件

## 1、文件的打开与关闭

**打开文件/创建文件**

在python中，使用open函数可以打开一个已经存在的文件，或者创建一个新的文件

open(文件路径，访问模式)

示例如下：

```python
f=open('test.txt','w')
```


| 选项 | 说明                                                                                                                                                     |
| ---- | :------------------------------------------------------------------------------------------------------------------------------------------------------- |
| r    | **以只读方式打开文件。**文件的指针将会放在文件的开头。这是默认模式。                                                                                     |
| w    | **打开一个文件只用于写入。**如果该文件已存在则将其覆盖。如果该文件不存在，创建新文件                                                                     |
| a    | **打开一个文件用于追加。**如果该文件已存在，文件指针将会放在文件的结尾。也就是说，新的内容将会被写入到已有内容之后。如果该文件不存在，创建新文件进行写入 |

**文件关闭**

对文件的操作进行完成后使用fp.close()对其进行关闭

## 2、文件的读写

**写数据**

```python
fp=open('test.txt','w')
fp.write('hello world')
fp.close()
```

**读数据**

```python
fp=open('test.txt','r')
content=fp.read()	#按照字节读取，效率较低
content=fp.readline()	#按照行读取，但是只能一行一行的读取
content=fp.readlines()	#按照行读取并且可以将所有的数据都读取到
```

## 3、序列化与反序列化

一、序列化与反序列化的定义

序列化：把python的数据类型转换成json格式的字符串类型。

反序列化：把json格式的字符类型串转换成python的数据类型。

二、作用

为了数据传输，在接口测试发送请求时使用的是json格式的字符串，需要进行序列化，

在实际的接口返回数据中，有各种类型，需要进行反序列化为python的数据类型，然后使用。

三、python中的json模块

在python中，可以使用json模块中的方法进行序列化和反序列化操作。

```python
# 两种序列化的方法
# dumps()
fp = open('test.txt','w')
# 默认状态下对象是无法写入到文件中去的
list = ['zhangsan','lisi']
# 将python对象变成json字符串
name = json.dumps(list)
fp.write(name)
fp.close()

# dump()
fp = open('test.txt','w')
list = ['zhangsan','lisi','wangwu']
json.dump(list,fp)
fp.close()


# 两种反序列化的方法
# loads()
fp = open('test.txt','r')
content = fp.read()
result = json.loads(content)

# load()
fp = open('test.txt','r')
result = json.load(fp)
print(type(result))
fp.close()
```

# urllib

## 1、urllib的基本使用

```python
import urllib.request

#使用urllib来访问百度首页
#定义一个url
url="http://www.baidu.com"
#模拟浏览器向服务器发送请求,得到相应结果
response=urllib.request.urlopen(url)
#获取相应中的页面源码
#read方法所返回的是字节形式的二进制数据，所以需要对其进行解码
print(response.read().decode('utf-8'))
```

## 2、urllib的一个类型和几个方法

```python
import urllib.request

url='http://www.baidu.com'
response=urllib.request.urlopen(url)

#response是HTTPResponse类型的数据
type=type(response)
print(type)

#read里加数字代表读取的字节数
content=response.read(5)
print(content)

#返回状态码      如果是200说明没有问题，若是418之类的则存在问题
print(response.getcode())

#返回的是url地址
print(response.geturl())

#返回的是状态信息
print(response.getheaders())
```

## 3、urllib的下载方法

```python
import urllib.request

#下载网页
url_page='http://www.baidu.com'
#url代表的是下载的路径，filename代表的是文件名
urllib.request.urlretrieve(url_page,'baidu.html')

#下载图片
url_img='https://img0.baidu.com/it/u=3394852584,4163708711&fm=253&app=53&size=w500&n=0&g=0n&f=jpeg?sec=1669177864&t=59cbca48ed5dd9574c3a1f6e51239e90'
urllib.request.urlretrieve(url_img,'习近平.jpg')

#下载视频
url_movie='https://vd2.bdstatic.com/mda-njg2v8dtxh4yd0bh/sc/cae_h264/1665972315755820356/mda-njg2v8dtxh4yd0bh.mp4?v_from_s=hkapp-haokan-suzhou&auth_key=1666587868-0-0-7f759229b00717f4a2c56661b0ef8fc2&bcevod_channel=searchbox_feed&pd=1&cd=0&pt=3&logid=2068511707&vid=9410008365167324209&abtest=104959_1&klogid=2068511707'
urllib.request.urlretrieve(url_movie,'二十大.mp4')

'''
通过检查视频网页的源代码从而确定视频所在的路径位置
<video class="art-video" preload="metadata" crossorigin="anonymous" autoplay="" src="https://vd2.bdstatic.com/mda-njg2v8dtxh4yd0bh/sc/cae_h264/1665972315755820356/mda-njg2v8dtxh4yd0bh.mp4?v_from_s=hkapp-haokan-suzhou&auth_key=1666587868-0-0-7f759229b00717f4a2c56661b0ef8fc2&bcevod_channel=searchbox_feed&pd=1&cd=0&pt=3&logid=2068511707&vid=9410008365167324209&abtest=104959_1&klogid=2068511707"></video>
'''
```

## 4、请求对象的定制

```
UA介绍：User Agent中文名为用户代理，它标识了你自己的一些电脑信息，一些特征字符串，说白了，就是包含了，用户信息，应用类型，操作系统，软件版本号，这些信息组合起来的一个字符串信息。
```

```python
import urllib.request

url='https://www.baidu.com'
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36 Edg/106.0.1370.52'
}
#因为urlopen方法不能存储字典类型，所以需要用得到请求对象的定制
#因为Requests参数顺序的问题不能直接填写url和headers，所以需要关键字来传递参数
request=urllib.request.Request(url=url,headers=headers)

response=urllib.request.urlopen(request)
print(response.read().decode('utf-8'))
```

## 5、编解码

### 1、get请求方式：urllib.parse.quote()

```python
import urllib.request
import urllib.parse
#获取网页源码
url='https://cn.bing.com/search?q='
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36 Edg/106.0.1370.52'
}

#将文字变为Unicode编码的格式，需要用到urllib.parse.quote方法
name=urllib.parse.quote('周杰伦')
url=url+name

#请求对象的定制
request=urllib.request.Request(url=url,headers=headers)
#模拟浏览器向服务器发送请求
response=urllib.request.urlopen(request)
print(response.read().decode('utf-8'))
```

### 2、get请求方式：urllib.parse.urlencode()

```python
import urllib.parse
import urllib.request
#urlencode的应用场景为多个参数的时候
#https://www.baidu.com/s?wd=周杰伦&sex=男
data={
    'wd':'周杰伦',
    'sex':'男',
    'location':'中国'
}
base_url='https://www.baidu.com/s?'
data=urllib.parse.urlencode(data)
url=base_url+data       #拼接
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36 Edg/106.0.1370.52'
}
#请求对象的定制
request=urllib.request.Request(url=url,headers=headers)
#模拟浏览器向服务器发送请求
response=urllib.request.urlopen(request)/
print(response.read().decode('utf-8'))
```

### 3、post请求方式

```python
import json
import urllib.request
import urllib.parse
url='https://fanyi.baidu.com/v2transapi?from=en&to=zh'
headers={
    'Accept':' */*',
    # 'Accept-Encoding':' gzip, deflate, br',解码方式要按照utf-8
    # 'Accept-Language':' zh-CN,zh;q=0.9',
    # 'Connection':' keep-alive',
    # 'Content-Length':' 135',
    # 'Content-Type':' application/x-www-form-urlencoded; charset=UTF-8',
    'Cookie':' BIDUPSID=77D04A5C24433FE840B4CB1679842A74; PSTM=1615373036; __yjs_duid=1_1370568de110b0962fabdc21d6911d6c1618068150020; BAIDUID=000D79B7F250CEFC5143DAC155AE483F:FG=1; BDUSS=xVeTJheUhBNXZpTFdhcFJmcVdSY0ZiaU50VkZPMVpOcHlHVEpYOVJuTi1VVTFqSVFBQUFBJCQAAAAAAAAAAAEAAABL4z9gbWFzdGVyYmFrAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAH7EJWN-xCVjM2; MCITY=-250%3A; BDUSS_BFESS=xVeTJheUhBNXZpTFdhcFJmcVdSY0ZiaU50VkZPMVpOcHlHVEpYOVJuTi1VVTFqSVFBQUFBJCQAAAAAAAAAAAEAAABL4z9gbWFzdGVyYmFrAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAH7EJWN-xCVjM2; APPGUIDE_10_0_2=1; REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; BDORZ=FFFB88E999055A3F8A630C64834BD6D0; BAIDUID_BFESS=000D79B7F250CEFC5143DAC155AE483F:FG=1; BA_HECTOR=a42lal04ak840la08g0kbu4b1hlnljq1b; ZFY=ZyR4Aq1W7q5MOd3nEuvm2LFALQaVWEHRGbHoX3d8FcU:C; RT="z=1&dm=baidu.com&si=cfppt2505p8&ss=l9sirr5j&sl=4&tt=57v&bcn=https%3A%2F%2Ffclog.baidu.com%2Flog%2Fweirwood%3Ftype%3Dperf&ld=5df&ul=6e7&hd=6fn"; BDRCVFR[S_ukKV6dOkf]=mk3SLVN4HKm; delPer=0; PSINO=3; H_PS_PSSID=36544_37636_37516_36921_37584_36885_34813_37627_36805_36786_37537_37497_37576_26350_22158; Hm_lvt_64ecd82404c51e03dc91cb9e8c025574=1666712975,1667017122; Hm_lpvt_64ecd82404c51e03dc91cb9e8c025574=1667017138; ab_sr=1.0.1_NjYzM2NlOTQ1NDJjNTA3N2FlMGM0YTJiYjI2MjBjZGRhNDI4NThjYmNiMWNmMDRkYTgzZmE5MGM3ZGI2NmMxYTY1Y2NjMzM5MDM2MDIzZGUyYTdlOTA4YzNkOTYyN2FiZWI1MzQ2NzFkYjQ2M2I3ODdiMjYxNTYyZmMwMjRhNjc2ZTUwMzg2ZmI1NTkyOTlhODA5NjlkNGY4MTc1OGVmYmE3MDE4YTNiNDhhNTRhN2E0ZTkxZWNmZGVmYzAyZjcw',
    # 'Host':' fanyi.baidu.com',
    # 'Origin':' https://fanyi.baidu.com',
    # 'Referer':' https://fanyi.baidu.com/translate?query=&keyfrom=baidu&smartresult=dict&lang=auto2zh',
    # 'sec-ch-ua':' ";Not A Brand";v="99", "Chromium";v="94"',
    # 'sec-ch-ua-mobile':' ?0',
    # 'sec-ch-ua-platform':' "Windows"',
    # 'Sec-Fetch-Dest':' empty',
    # 'Sec-Fetch-Mode':' cors',
    # 'Sec-Fetch-Site':' same-origin',
    # 'User-Agent':' Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36 Core/1.94.178.400 QQBrowser/11.2.5170.400',
    # 'X-Requested-With':' XMLHttpRequest'
}
data={
    'from':'en',
    'to':'zh',
    'query':'love',
    'transtype':'realtime',
    'simple_means_flag':'3',
    'sign':'198772.518981',
    'token':'a3115c6f1adbd0f5b2280e6935929ca8',
    'domain':'common'
}
#post请求的参数必须进行编码并且要调用encode编码
data=urllib.parse.urlencode(data).encode('utf-8')
#请求对象的定制
request=urllib.request.Request(url=url,data=data,headers=headers)
#模拟浏览器发送请求给服务器
response=urllib.request.urlopen(request)
content=response.read().decode('utf-8')
obj=json.loads(content)
print(obj)
```

## 6、ajax的get请求（豆瓣）

```python
import urllib.request
import urllib.parse

def creat_request(page):
    base_url='https://movie.douban.com/j/chart/top_list?type=6&interval_id=100%3A90&action=&'
    data={
        'start':(page-1)*20,
        'limit':20
    }
    data=urllib.parse.urlencode(data)
    url=base_url+data
    headers={
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'
    }
    requests = urllib.request.Request(url=url,headers=headers)
    return requests

def get_content(requests):
    response = urllib.request.urlopen(requests)
    content = response.read().decode('utf-8')
    return content

def down_load(page,content):
    with open('douban'+str(page)+'.json','w',encoding='utf-8') as fp:
        fp.write(content)

# 程序的入口
if __name__ == '__main__':
    start_page = int(input('起始页码：'))
    end_page = int(input('结束页码：'))
    for page in range(start_page,end_page+1):
        # 每一页请求对象的定制
        requests = creat_request(page)
        # 获取响应的数据
        content = get_content(requests)
        # 下载
        down_load(page,content)
```

## 7、ajax的post请求（KFC）

```python
X-Requested-With: XMLHttpRequest存在则说明需要用到ajax
```

```python
import urllib.request
import urllib.parse

def creat_request(page):
    base_url='http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname'
    data={
        'cname': '合肥',
        'pid': '',
        'pageIndex': page,
        'pageSize': '10'
    }
    data=urllib.parse.urlencode(data).encode('utf-8')
    headers={
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'
    }
    requests = urllib.request.Request(url=base_url,data=data,headers=headers)
    return requests

def get_content(requests):
    response = urllib.request.urlopen(requests)
    content = response.read().decode('utf-8')
    return content

def down_load(page,content):
    with open('kfc'+str(page)+'.json','w',encoding='utf-8') as fp:
        fp.write(content)


# 程序的入口
if __name__ == '__main__':
    start_page = int(input('起始页码：'))
    end_page = int(input('结束页码：'))
    for page in range(start_page,end_page+1):
        # 每一页请求对象的定制
        requests = creat_request(page)
        # 获取响应的数据
        content = get_content(requests)
        # 下载
        down_load(page, content)
```

## 8、URLError\HTTPError

httperror:当我们向服务器发出请求时，服务器会产生response请求，如果urlopen不能处理则爆出httperror异常
httperror的父类是urlerror异常
urlerror:产生的原因主要是1.网络没有连接，2服务器连接失败，3，找不到指定的服务器。

```python
import urllib.request
import urllib.error
url = 'https://blog.csdn.net/m0_52634705/article/details/116187237'     # 出现http的错误，比如后面数字出现变动
url = 'https://www.goudan111.com'   # 出现网页url的错误
headers={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36'
}

try:
    requests = urllib.request.Request(url=url,headers=headers)
    response = urllib.request.urlopen(requests)
    content = response.read().decode('utf-8')
    print(content)
except urllib.error.HTTPError:
    print("服务器正在升级，请稍后！！")
except urllib.error.URLError:
    print("出了一点小差错!!")
```

## 9、cookie登录

```python
import urllib.request
# 适用场景：数据采集的时候需要绕过登录进入到某个页面
# 个人信息页面是utf-8,但还是报了编码错误,是因为没有进入个人信息页面是跳转到登录页面
# 因为登录页面并不是utf-8所以会报编码错误
url = 'https://weibo.cn/7508933403/info'
headers={
    # cookie中携带着登录信息，如果有登录之后的cookie那么我们就可以使用该cookie进入各种页面
    'cookie':'_T_WM=76480021925; SCF=ArO-W0xIkMp7q8-8o1XoLj39i1Jh3MSb3DHAgTjg0O2cOLm9uImFWGp2qOJUNDvvIEUnpLEU_zwy9xKVWx9A6Nc.; SUB=_2A25OYzbxDeRhGeFL61oY8y3Iyz-IHXVtrFq5rDV6PUJbktANLXfbkW1NQp9S0IcVyhqWA84mcLQvz1h-00MGtT7X; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9WFFwVDMfj.6Ji11G.oVF1x-5JpX5oz75NHD95QNSK5R1Ke0Sh50Ws4DqcjMi--NiK.Xi-2Ri--ciKnRi-zNS0-71h.0e0B7e7tt; ALF=1670304673; MLOGIN=1',
    # referer 用来判断当前路径是不是由上一级路径进来的，可以用来作为图片的防盗链
    'referer':'https://weibo.cn/',
    'user-agent':' Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36 Edg/107.0.1418.35'
}
requests = urllib.request.Request(url=url,headers=headers)
response = urllib.request.urlopen(requests)
content = response.read().decode('utf-8')
# 将数据保存到本地
with open('weibo.html','w',encoding='utf-8') as fp:
    fp.write(content)
```

## 10、Handler处理器

```python
import urllib.request
import urllib.error

url='http://www.baidu.com'
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36 Edg/106.0.1370.52'
}

request = urllib.request.Request(url=url, headers=headers)
# 获取handler对象
handler = urllib.request.HTTPHandler()
# 获取opener对象
opener = urllib.request.build_opener(handler)
# 调用open方法
response = opener.open(request)
content = response.read().decode('utf-8')
print(content)
```

## 11、代理服务器

**代理的常用功能**
1、突破自身IP访问限制，访问国外站点

2、访问一些单位或团体的内部资源

```
拓展：如某大学FTP（前提是该代理地址在该资源的允许访问范围之内），使用教育网内地址段免费代理服务器，就可以用于对教育网开放的各类FTP下载上传，以及各类资料查询共享等服务。
```


3、提高访问速度

```
拓展：通常代理服务器都设置一个较大的硬盘缓冲区，当有外界的信息通过时，同时也将其保存到缓冲区中，当其他用户再访问相同的信息时，则直接由缓冲区中取出信息，传给用户，以提高访问速度。
```


4、隐藏真实IP

```
拓展：上网者也可以通过这种方法隐藏自己的IP，免受攻击。
```


```python
import random
import urllib.request

# 代理池
proxies_pool = [
    {'http':'112.14.47.6:52024'},
    {'http':'117.114.149.66:55443'},
    {'http':'121.13.252.62:41564'},
    {'http':'121.13.252.61:41564'}
]
proxies = random.choice(proxies_pool)
print(proxies)

url='https://www.baidu.com/s?tn=49055317_28_hao_pg&ie=utf-8&wd=ip'
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36 Edg/106.0.1370.52'
}

request = urllib.request.Request(url=url,headers=headers)
handler = urllib.request.ProxyHandler(proxies=proxies)
opener = urllib.request.build_opener(handler)
response = opener.open(request)
content = response.read().decode('utf-8')
with open('ip.html','w',encoding='utf-8') as fp:
    fp.write(content)
```

# 解析

## 1、xpath

```
xpath的基本语法
	1、路径查询
		//:查找所有的子孙节点，不考虑层级关系
		/ :找直接子节点
	2、谓词查询
		//div[@id]
		//div[@id="maincontent"]
	3、属性查询
		//@class
	4、模糊查询(包含查询，以……为开始的查询)
		//div[contains(@id,"he")]
		//div[starts-with(@id,"he")]
	5、内容查询
		//div/h1/text()
	6、逻辑运算(且，或)
		//div[@id="head" and @class="s_down"]
		//title | //price
```

### **xpath的基本使用**

```python
from lxml import etree

# xpath解析
# 本地文件            etree.parse
# 服务器响应数据       etree.HTML

# xpath来解析本地文件
tree = etree.parse('14_解析_xpath的基本使用.html')

# tree.xpath('xpath路径')

# 查找ul下面的li
li_list = tree.xpath('//body/ul/li')

# 查找所有具有id属性的标签,使用text()可以查询标签中的内容
id_list = tree.xpath('//ul/li[@id="l1"]/text()')

# 查找id为l1的li标签的class属性值
li_class = tree.xpath('//ul/li[@id="l1"]/@class')

# 模糊查询id中包含l的li标签
mohu_list = tree.xpath('//ul/li[contains(@id,"l")]')

# 判断列表的长度
print(li_list,id_list,li_class,mohu_list)
```

### **xpath案例——获取百度网页指定内容**

```python
# （1）获取网页的源码
import urllib.request
url = 'https://www.baidu.com'
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'
}
requests = urllib.request.Request(url=url,headers=headers)
reponse = urllib.request.urlopen(requests)
content = reponse.read().decode('utf-8')
# （2）解析服务器响应的文件
from lxml import etree
# 解析服务器响应文件     xpath的返回值是一个列表形式的数据
tree = etree.HTML(content)
result = tree.xpath('//input[@id="su"]/@value')
print(result) 
```

### **xpath综合案例——爬取网页图片资源**

**普通版**

```python
import urllib.request
from lxml import etree

def creat_request(page):
    if (page == 1):
        url = 'https://pic.netbian.com/4kmeinv/index.html'
    else:
        url = 'https://pic.netbian.com/4kmeinv/index_'+str(page)+'.html'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'
    }
    requests = urllib.request.Request(url=url,headers=headers)
    return requests

def get_content(requests):
    response = urllib.request.urlopen(requests)
    content = response.read().decode('gbk')
    return content

def download(content):
    tree = etree.HTML(content)
    # urllib.request.urlretrieve('图片地址','文件名字')
    src_list = tree.xpath('//div[@id="main"]//img/@src')
    name_list = tree.xpath('//div[@id="main"]//img/@alt')
    for i in range(len(name_list)):
        name = name_list[i]
        url = 'https://pic.netbian.com' + src_list[i]
        print(name+'下载完成')
        urllib.request.urlretrieve(url=url,filename='E:/mm/'+name+'.jpg')

if __name__ == '__main__':
    start_page = int(input('起始页码：'))
    end_page = int(input('结束页码：'))
    for page in range(start_page,end_page+1):
        # 请求对象的定制
        requests = creat_request(page)
        # 获取网页源码
        content = get_content(requests)
        # 下载
        download(content)
```

**高清修正版**

```python
import urllib.request
from lxml import etree

def get_newurl(page):
    if (page == 1):
        url = 'https://pic.netbian.com/4kmeinv/index.html'
    else:
        url = 'https://pic.netbian.com/4kmeinv/index_'+str(page)+'.html'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'
    }
    requests = urllib.request.Request(url=url,headers=headers)
    response = urllib.request.urlopen(requests)
    content = response.read().decode('gbk')
    tree = etree.HTML(content)
    newurl_list = tree.xpath('/html/body/div[2]/div/div[3]/ul//a/@href')
    return newurl_list

def download(new_url):
    for i in range(len(new_url)):
        url = 'https://pic.netbian.com' + new_url[i]
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36'
        }
        requests = urllib.request.Request(url=url,headers=headers)
        response = urllib.request.urlopen(requests)
        content = response.read().decode('gbk')
        tree = etree.HTML(content)
        url = tree.xpath('/html/body/div[2]/div[1]/div[2]/div[1]/div[2]/a/img/@src')
        download_url = 'https://pic.netbian.com' + url[0]
        name = tree.xpath('/html/body/div[2]/div[1]/div[2]/div[1]/div[2]/a/img/@title')
        urllib.request.urlretrieve(url=download_url,filename='E:/mm/'+name[0]+'.jpg')

if __name__ == '__main__':
    start_page = int(input('起始页码：'))
    end_page = int(input('结束页码：'))
    for page in range(start_page,end_page+1):
        new_url = get_newurl(page)
        download(new_url)
```

## 2、jsonpath

参考文献：https://blog.csdn.net/luxideyao/article/details/77802389


| **XPath** | **JSONPath**       | **Description**                                                     |
| --------- | ------------------ | ------------------------------------------------------------------- |
| /         | $                  | 表示根元素                                                          |
| .         | @                  | 当前元素                                                            |
| /         | . 或[]             | 子元素                                                              |
| ..        | n/a                | 父元素                                                              |
| //        | ..                 | 递归下降，JSONPath是从E4X借鉴的。                                   |
| *         | *                  | 通配符，表示所有的元素                                              |
| @         | n/a                | 属性访问字符                                                        |
| []        | []                 | 子元素操作符                                                        |
| \|        | [,]                | 连接操作符在XPath 结果合并其它结点集合。JSONP允许name或者数组索引。 |
| n/a       | [start：end：step] | 数组分割操作从ES4借鉴。                                             |
| []        | ?()                | 应用过滤表示式                                                      |
| n/a       | ()                 | 脚本表达式，使用在脚本引擎下面。                                    |
| ()        | n/a                | Xpath分组                                                           |

案例示范：


| **XPath**            | **JSONPath**              | **结果**                               |
| -------------------- | ------------------------- | -------------------------------------- |
| /store/book/author   | $.store.book[*].author    | 书点所有书的作者                       |
| //author             | $..author                 | 所有的作者                             |
| /store/*             | $.store.*                 | store的所有元素。所有的bookst和bicycle |
| /store//price        | $.store..price            | store里面所有东西的price               |
| //book[3]            | $..book[2]                | 第三个书                               |
| //book[last()]       | $..book[(@.length-1)]     | 最后一本书                             |
| //book[position()<3] | $..book[0,1]``$..book[:2] | 前面的两本书。                         |
| //book[isbn]         | $..book[?(@.isbn)]        | 过滤出所有的包含isbn的书。             |
| //book[price<10]     | $..book[?(@.price<10)]    | 过滤出价格低于10的书。                 |
| //*                  | $..*                      | 所有元素。                             |

```python
import json
import jsonpath
import urllib.request

url = 'https://dianying.taobao.com/cityAction.json?activityId&_ksTS=1668268145775_108&jsoncallback=jsonp109&action=cityAction&n_s=new&event_submit_doGetAllRegion=true'
headers = {
    'accept':' text/javascript, application/javascript, application/ecmascript, application/x-ecmascript, */*; q=0.01',
    # 'accept-encoding':' gzip, deflate, br',
    'accept-language':' zh-CN,zh;q=0.9',
    'bx-v':' 2.2.3',
    'cookie':' _m_h5_tk=f6663cc7b65068c9813d6b1762d661a1_1668192010205; _m_h5_tk_enc=fada52344c5a231f01033c0d0dd4f0ad; cna=HJzwG40R5l8CAdONrJu6dbD1; xlly_s=1; t=8232d1945a237d542009fa5f62ac94ba; cookie2=1c435f70fd1b44c570abce085193228b; v=0; _tb_token_=e364515b9fe6d; tb_city=340400; tb_cityName="u7TEzw=="; tfstk=c_LhBkDb9HSQFHgJCp_IldJiFbiAaMoFdU8w7fAmFM8BqJYNTs4bboy83fXKhvr5.; l=eBM4s7mgTyl0TM9fBO5Courza77OzIObzPVzaNbMiInca6MdteliRNCUxMYpSdtjgtfeFetzmSrNYdBmA3Up_giMW_9jisV6mY9w-; isg=BEpKIr-XHeowXpFHdxCT_rKvmzDsO86V3p4LtNSCph02h-hBvMtypMD5l_NbckYt',
    'referer':' https://dianying.taobao.com/index.htm?spm=a1z21.3046609.city.123.2e0d112aLJpyuc&n_s=new&city=340400',
    'sec-ch-ua':' "Google Chrome";v="107", "Chromium";v="107", "Not=A?Brand";v="24"',
    'sec-ch-ua-mobile':' ?0',
    'sec-ch-ua-platform':' "Windows"',
    'sec-fetch-dest':' empty',
    'sec-fetch-mode':' cors',
    'sec-fetch-site':' same-origin',
    'user-agent':' Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',
    'x-requested-with':' XMLHttpRequest'
}
requests = urllib.request.Request(url=url,headers=headers)
response = urllib.request.urlopen(requests)
content = response.read().decode('utf-8')
# 得到的json文件多出了不必要的部分
# 使用切片的方法得到有效数据
content = content.split('(')[1].split(')')[0]
# with open('17_解析_jsonpath解析淘票票.json','w',encoding='utf-8') as fp:
#     fp.write(content)

obj = json.load(open('17_解析_jsonpath解析淘票票.json','r',encoding='utf-8'))
id = jsonpath.jsonpath(obj,'$..id')
city = jsonpath.jsonpath(obj,'$..regionName')
code = jsonpath.jsonpath(obj,'$..cityCode')
pinyin = jsonpath.jsonpath(obj,'$..pinYin')

# 将list按列存入csv
import csv
rows = zip(id,city,code,pinyin)
with open('城市数据.csv', "w", newline='') as f:
    writer = csv.writer(f)
    for row in rows:
        writer.writerow(row)
```

## 3、BeautifulSoup

```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(open('17_解析_bs4的基本使用.html',encoding='utf-8'),'lxml')

# 根据标签名来查找节点
# 找到的是第一个符合条件的数据
print(soup.a)
# 获取标签的属性和属性值
print(soup.a.attrs)

# bs4的一些函数
# （1）find 返回的是第一个符合条件的数据
print(soup.find('a'))
# 根据title的值找到对应的标签对象
print(soup.find('a',title='百度'))
# class属性比较特殊需要加上下划线
print(soup.find('a',class_='l1'))

# （2）find_all 返回的是一个列表其中包含了所有的标签
print(soup.find_all('a'))
# 如果想要获取到是含有多个标签的数据，那么就需要给所有的标签以列表的形式输入
print(soup.find_all(['a','span']))

# （3）select
# select返回的是一个列表，并且返回的是多个数据
print(soup.select('a'))
# 可以通过.来代表class 我们把这种叫做类选择器
print(soup.select('.l1'))
# 使用#来代表id
print(soup.select('#l2'))
```

# selenium

## 1、selenium的基本使用

```python
from selenium import webdriver

# 创建浏览器操作对象
browser = webdriver.Chrome()
# 访问网站
url = 'https://www.jd.com'
browser.get(url)
# page_source获取网页源码
content = browser.page_source
print(content)
```

## 2、selenium4的元素定位写法

```python
browser.find_elements(By.CLASS_NAME,"xx")
browser.find_elements(By.CSS_SELECTOR,"xx")
browser.find_elements(By.ID,"xx")
browser.find_elements(By.LINK_TEXT,"xx")
browser.find_elements(By.NAME,"xx")
browser.find_elements(By.PARITIAL_LINK_TEXT,"xx")
browser.find_elements(By.TAG_NAME,"xx")
browser.find_elements(By.XPATH,"xx")
```

**写法实例**

```python
from selenium import webdriver
from selenium.webdriver.common.by import By

browser = webdriver.Chrome()
url = 'https://www.baidu.com'
browser.get(url)
# 元素定位
# 根据id来找到对象
botton1 = browser.find_element(By.ID,'su')
# 根据标签属性的属性值来获取对象
botton2 = browser.find_element(By.NAME,'wd')
# 根据xpath来定位对象
botton3 = browser.find_element(By.XPATH,'//input[@id="su"]')
# 根据标签名来获取对象
botton4 = browser.find_elements(By.TAG_NAME,'input')
# 使用bs4的语法来实现定位
botton5 = browser.find_elements(By.CSS_SELECTOR,'#su')
# 使用链接文本来实现定位
botton6 = browser.find_elements(By.LINK_TEXT,'地图')
print(botton1,botton2,botton3,botton4,botton5,botton6)
```

## 3、selenium元素信息交互

**访问元素信息：**

```python
获取元素属性：
	.get_attribute('class')
获取元素文本：
	.text
获取标签名：
	.tag_name
```

```python
browser = webdriver.Chrome()
url = 'https://www.baidu.com'
browser.get(url)

input = browser.find_element(By.ID,'su')
# 获取标签属性
print(input.get_attribute('value'))
# 获取标签名
print(input.tag_name)
# 获取元素文本
a = browser.find_element(By.LINK_TEXT,'新闻')
print(a.text)
```

**交互：**https://blog.csdn.net/weixin_48626846/article/details/125564074

```python
点击：click()

输入：send_keys('xxx')

后退操作：back()

前进操作：forward()

刷新：refresh()

模拟js滚动：
# 模拟鼠标滚轮，滑动页面至底部
js = "window.scrollTo(0, document.body.scrollHeight)" 
driver.execute_script(js)
# 模拟鼠标滚轮，滑动页面至顶部
js = "window.scrollTo(0, 0)"
driver.execute_script(js)
# 滑动到具体位置
driver.execute_script("window.scrollTo(x, y)")
js = "window.scrollBy(0, 500)"  # 向下滑动500个像素
js = "window.scrollBy(0, -500)"　# 向上滚动500个像素
js = "window.scrollBy(500, 0)"  # 向右滑动500个像素
js = "window.scrollBy(-500, 0)"　# 向左滚动500个像素

获取网页源码：page_source
```

# requests

## 1、requests的基本使用

帮助文档：https://cn.python-requests.org/zh_CN/latest/

快速上手：https://cn.python-requests.org/zh_CN/latest/user/quickstart.html

```python
url="http://www.baidu.com"
requests = requests.get(url)
# 一个类型和六个属性
# Response类型
print(type(requests))
# 设置相应编码的格式
requests.encoding = 'utf-8'
# 以字符串的形式返回网页源码
print(requests.text)
# 返回url地址
print(requests.url)
# 返回二进制数据
print(requests.content)
# 返回状态码
print(requests.status_code)
# 返回头信息
print(requests.headers)
```

## 2、requests的get请求

```python
import requests
url = 'https://www.baidu.com/s?'
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',
    'Cookie':'BAIDUID=CD77FFAA4C039C3D056196E322D1A02E:FG=1; BIDUPSID=FC23AE2795F13777E73E9B7334A265AB; PSTM=1670573728; BD_UPN=12314753; BDUSS=9XQkFNYjJ5V0htQlVCdGJKaW96djFZd200ZEdnazRTR0JZZWxsajhEQXJmcnRqSVFBQUFBJCQAAAAAAAAAAAEAAABL4z9gbWFzdGVyYmFrAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACvxk2Mr8ZNjMW; BDUSS_BFESS=9XQkFNYjJ5V0htQlVCdGJKaW96djFZd200ZEdnazRTR0JZZWxsajhEQXJmcnRqSVFBQUFBJCQAAAAAAAAAAAEAAABL4z9gbWFzdGVyYmFrAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACvxk2Mr8ZNjMW; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; H_PS_PSSID=37855_36558_37690_37909_37623_37795_36802_37949_37938_37758_37904_26350_37790_37881; ab_sr=1.0.1_MGRiN2I0NTYxNGJjMzZlNmIwZTVkZTcyZDMwMGJhYTM3ZjhkMWJjN2ZiODk0ZDY2MTU1ODVlZWI0ZTUxYTBlYTY1M2U0ZTY4ZmMyYjc1MWNhYmY4OGY4YzY5YTNhY2Q3NGY1OTU2NDlhYmU4NjhiZTFhODk0OTRkMTg5MmIwMTAyZjVlZjg1MjVmYTA1Mjk0ZjIzZTk3MTFiMjI3ODZhZGViNzU3MzZlMDg3NTE5YjdjMzUzMDY0ODA3MDhiNDE5; BAIDUID_BFESS=CD77FFAA4C039C3D056196E322D1A02E:FG=1; delPer=0; BD_CK_SAM=1; PSINO=3; BA_HECTOR=ak2k2kah04250l8k25250m7d1hpql191g; ZFY=zw5W8LQ1bdm9TSXZnmHqBYPTiYrGTfacahbq0OG2mNE:C; B64_BOT=1; H_PS_645EC=d7abYdKSQxVCBGVRIb54LW6WFaE%2FWbPK2l0kM8EPbFbtW4jyNMgxM0YrPgY; baikeVisitId=a5799b48-cb1f-4e55-a4f5-c645e4b86090; BDSVRTM=27'
}
data = {
    'wd':'北京'
}
# url为路径，params为参数
response = requests.get(url=url,params=data,headers=headers)
response.encoding = 'utf-8'
print(response.text)

# 总结：
# （1）参数使用params传递
# （2）参数无需urlencode编码
# （3）不需要请求对象的定制
```

## 3、requests的post请求

```python
import json
import requests
post_url = 'https://fanyi.baidu.com/sug'
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'
}
data = {
    'kw':'eye'
}
response = requests.post(url=post_url,headers=headers,data=data)
content = response.text
obj = json.loads(content)
print(obj)

# 总结：
# （1）post请求不需要编解码
# （2）post请求参数是data
# （3）不需要请求对象的定制
```

## 4、requests的代理

```python
url = 'https://www.baidu.com/s?'
headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',
    'Cookie':'BAIDUID=CD77FFAA4C039C3D056196E322D1A02E:FG=1; BIDUPSID=FC23AE2795F13777E73E9B7334A265AB; PSTM=1670573728; BD_UPN=12314753; BDUSS=9XQkFNYjJ5V0htQlVCdGJKaW96djFZd200ZEdnazRTR0JZZWxsajhEQXJmcnRqSVFBQUFBJCQAAAAAAAAAAAEAAABL4z9gbWFzdGVyYmFrAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACvxk2Mr8ZNjMW; BDUSS_BFESS=9XQkFNYjJ5V0htQlVCdGJKaW96djFZd200ZEdnazRTR0JZZWxsajhEQXJmcnRqSVFBQUFBJCQAAAAAAAAAAAEAAABL4z9gbWFzdGVyYmFrAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACvxk2Mr8ZNjMW; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; H_PS_PSSID=37855_36558_37690_37909_37623_37795_36802_37949_37938_37758_37904_26350_37790_37881; ab_sr=1.0.1_MGRiN2I0NTYxNGJjMzZlNmIwZTVkZTcyZDMwMGJhYTM3ZjhkMWJjN2ZiODk0ZDY2MTU1ODVlZWI0ZTUxYTBlYTY1M2U0ZTY4ZmMyYjc1MWNhYmY4OGY4YzY5YTNhY2Q3NGY1OTU2NDlhYmU4NjhiZTFhODk0OTRkMTg5MmIwMTAyZjVlZjg1MjVmYTA1Mjk0ZjIzZTk3MTFiMjI3ODZhZGViNzU3MzZlMDg3NTE5YjdjMzUzMDY0ODA3MDhiNDE5; BAIDUID_BFESS=CD77FFAA4C039C3D056196E322D1A02E:FG=1; delPer=0; BD_CK_SAM=1; PSINO=3; BA_HECTOR=ak2k2kah04250l8k25250m7d1hpql191g; ZFY=zw5W8LQ1bdm9TSXZnmHqBYPTiYrGTfacahbq0OG2mNE:C; B64_BOT=1; H_PS_645EC=d7abYdKSQxVCBGVRIb54LW6WFaE%2FWbPK2l0kM8EPbFbtW4jyNMgxM0YrPgY; baikeVisitId=a5799b48-cb1f-4e55-a4f5-c645e4b86090; BDSVRTM=27'
}
data = {
    'wd':'ip'
}
proxies_pool = [
    {'http':'112.14.47.6:52024'},
    {'http':'117.114.149.66:55443'},
    {'http':'121.13.252.62:41564'},
    {'http':'121.13.252.61:41564'}
]
proxies = random.choice(proxies_pool)
response = requests.get(url=url,params=data,headers=headers,proxies=proxies)
content = response.text
with open('代理.html','w',encoding='utf-8') as fp:
    fp.write(content)
```

## 5、requests的cookie登录

```python
url = 'https://www.ptpress.com.cn/login'     # 表单提交入口
# 使用Cookie登录方法实现模拟登录lh*20030304*
cookie = 'acw_tc=2760778616712630332746586ecfa3a2f5b580d1d1adc2abff6bd48abe35ef; JSESSIONID=F0905F1CB92F3628BE5731A0863E2432; gr_user_id=227fc491-df59-465e-b51b-e2f1348f3eb8; gr_session_id_9311c428042bb76e=361c223f-ce3c-4972-8d96-fa7addd35e51; gr_session_id_9311c428042bb76e_361c223f-ce3c-4972-8d96-fa7addd35e51=true'
Cookies = {}

# 将Cookie转成字典结构
for i in cookie.split(';'):
    key, value = i.split('=')
    Cookies[key] = value

host = 'https://www.ptpress.com.cn/'        # 网站主页
rq = requests.get(host, cookies=Cookies)    # 以登录状态获取目标网站数据
print(rq.text)
```

# scrapy

菜鸟教程：https://www.runoob.com/w3cnote/scrapy-detail.html

## 1、scrapy项目创建与运行

```
1、创建爬虫的项目
scrapy startproject xxx
注意：项目的名称不能用数字开头，也不能包含中文

2、创建爬虫文件
要在spiders文件夹中创建爬虫文件
scrapy genspider xxx url
eg:scrapy genspider baidu www.baidu.com

3、运行爬虫代码
scrapy crawl xxx
eg:scrapy crawl baidu
注意：如果运行爬取不成功需要在原有的settings.py里去将robots协议关掉
```

## 2、scrapy项目的结构

```
1️⃣ Spiders文件夹：这文件夹我们不陌生，因为每一次新建scrapy爬虫项目后，我们都需要终端进入Spiders文件夹，生产爬虫文件。在Spiders文件夹下，又有两个文件，一个是_init_.py文件，一个是tc.py。_init_.py文件是我们创建项目时默认生成的一个py文件，我们用不到这个py文件，因此我们可以忽略它，另一个tc.py文件是我们爬虫的核心文件，后续的大部分代码都会写入这个文件，因此它是至关重要的py文件。

2️⃣_init_.py文件：它和上面提到的Spiders文件夹下的_init_.py一样，都是不被使用的py文件，无需理会。

3️⃣ items.py文件：这文件定义了数据结构，这里的数据结构与算法中的数据结构不同，它指的是爬虫目标数据的数据组成结构，例如我们需要获取目标网页的图片和图片的名称，那么此时我们的数据组成结构就定义为 图片、图片名称。后续会专门安排对scrapy框架定义数据结构的学习。

4️⃣ middleware.py文件：这py文件包含了scrapy项目的一些中间构件，例如代理、请求方式、执行等等，它对于项目来说是重要的，但对于我们爬虫基础学习来说，可以暂时不考虑更改它的内容。

5️⃣ pipelines.py文件：这是我们之前在工作原理中提到的scrapy框架中的管道文件，管道的作用是执行一些文件的下载，例如图片等，后续会安排对scrapy框架管道的学习，那时会专门研究这个py文件。

6️⃣ settings.py文件：这文件是整个scrapy项目的配置文件，里面是很多参数的设置，我们会偶尔设计到修改该文件中的部分参数，例如下一部分提到的ROBOTS协议限制，就需要进入该文件解除该限制，否则将无法实现爬取。
```

## 3、response的属性和方法

```
response.text	获取的是响应的字符串
response.body	获取的是二进制数据
response.xpath('xxx')	可以直接用xpath方法来解析response中的内容
response.extract()	提取seletor对象的data属性值
response.extract_first()	提取seletor列表的第一个数据
```

## 4、scrapy的工作原理

![img](https://www.runoob.com/wp-content/uploads/2018/10/8c591d54457bb033812a2b0364011e9c_articlex.png)

- **Scrapy Engine(引擎)**: 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。
- **Scheduler(调度器)**: 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。
- **Downloader（下载器）**：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理，
- **Spider（爬虫）**：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器).
- **Item Pipeline(管道)**：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方。
- **Downloader Middlewares（下载中间件）**：你可以当作是一个可以自定义扩展下载功能的组件。
- **Spider Middlewares（Spider中间件）**：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests

## 5、scrapy shell

语法：scrapy shell [url]

写法同scrapy

## 6、yield

yield是一个类似return的关键字，迭代一次遇到yield时就返回yield后面的值。

案例分析：爬取当当网多页数据

#### dang.py

```python
import scrapy
from ..items import ScrapyDangdangItem

class DangSpider(scrapy.Spider):
    name = 'dang'
    allowed_domains = ['search.dangdang.com']
    start_urls = ['http://search.dangdang.com/?key=%B4%F3%CA%FD%BE%DD&act=input']

    base_url = 'http://search.dangdang.com/?key=%B4%F3%CA%FD%BE%DD&act=input&page_index='
    page = 1

    def parse(self, response):
        print('==========================================')
        # src = response.xpath('//ul[@class="bigimg"]/li/a/img/@src')
        # name = response.xpath('//ul[@class="bigimg"]/li/a/img/@alt')
        # price = response.xpath('//ul[@class="bigimg"]/li/p[3]/span[1]/text()')
        li_list = response.xpath('//ul[@class="bigimg"]/li')
        for li in li_list:
            # 第一张图片使用的是src标签，其他的图片使用的是data-original标签
            src = li.xpath('.//a/img/@data-original').extract_first()# 懒加载选择这个标签
            if src: # 如果src不为空则继续使用src
                src = src
            else:   # 如果src为空则使用这个xpath路径
                src = li.xpath('.//a/img/@src').extract_first()
            name = li.xpath('.//a/img/@alt').extract_first()
            price = li.xpath('.//p[3]/span[1]/text()').extract_first()
            book = ScrapyDangdangItem(src=src,name=name,price=price)
            # 获取一个book信息就将其交给管道pipelines
            yield book

        if self.page < 100:
            self.page+=1
            url = self.base_url + str(self.page)
            # 调用parse方法
            # 这个就是scrapy的get请求,其中callback是需要执行的函数
            yield scrapy.Request(url=url,callback=self.parse)
```

#### items.py

```python
class ScrapyDangdangItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    src = scrapy.Field()
    name = scrapy.Field()
    price = scrapy.Field()
```

#### pipelines.py

```python
# 如果想使用pipelines就必须要在setting中调整设置
class ScrapyDangdangPipeline:
    # 在爬虫文件开始前所执行的方法
    def open_spider(self,spider):
        self.fp = open('book.json','a',encoding='utf-8')
    # item就是yield后面的book对象
    def process_item(self, item, spider):
        # # 以下的这种方式并不推荐因为对于文件的操作过于频繁
        # # write方法必须要是字符串，不能是其他类型
        # with open('book.json','a',encoding='utf-8') as fp:
        #     fp.write(str(item))
        self.fp.write(str(item))
        return item
    # 在爬虫文件执行完后执行的方法
    def close_spider(self,spider):
        self.fp.close()

# 多条管道开启
class DangDangDownloadPipeline:
    def process_item(self,item,spider):

        url = 'http:' + item.get('src')
        filename = './spiders/books/' + item.get('name') + '.jpg'
        urllib.request.urlretrieve(url=url,filename=filename)

        return item
```

#### setting.py

要使用管道就必须在设置文件里将pipelines注释掉

